{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End2end ConvGRU\n",
    "\n",
    "> Autoencoder + forecaster in the same training loop. Based on (https://github.com/tcapelle/moving_mnist/blob/master/01_train_example.ipynb) and (https://github.com/tcapelle/moving_mnist/blob/master/02_train_cross_entropy_loss-Copy1.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "from fastai.vision.all import *\n",
    "from mocatml.utils import *\n",
    "convert_uuids_to_indices()\n",
    "from mocatml.data import *\n",
    "from mocatml.models.conv_rnn import *\n",
    "from mygrad import sliding_window_view\n",
    "from tsai.imports import my_setup\n",
    "from tsai.utils import yaml2dict, dict2attrdict\n",
    "from fastai.callback.schedule import valley, steep\n",
    "from fastai.callback.wandb import WandbCallback\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.callback.schedule import LRFinder\n",
    "\n",
    "@patch_to(LRFinder)\n",
    "def after_fit(self):\n",
    "    self.learn.opt.zero_grad() # Needed before detaching the optimizer for future fits\n",
    "    tmp_f = self.path/self.model_dir/self.tmp_p/'_tmp.pth'\n",
    "    if tmp_f.exists():\n",
    "        self.learn.load(f'{self.tmp_p}/_tmp', with_opt=True, device='cpu')\n",
    "        self.tmp_d.cleanup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "config_base = yaml2dict('./config/base.yaml', attrdict=True)\n",
    "config_base.convgru = yaml2dict('./config/convgru/convgru.yaml', attrdict=True)\n",
    "#config = AttrDict({**config_base, **config_e2e})\n",
    "config = AttrDict(config_base)\n",
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set device\n",
    "default_device(0 if config.device == 'cpu' else config.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run = wandb.init(dir=ifnone(config.wandb.dir, '../'),\n",
    "                 project=config.wandb.project, \n",
    "                 config=config,\n",
    "                 group=config.wandb.group,\n",
    "                 mode=config.wandb.mode, \n",
    "                 anonymous='never') if config.wandb.enabled else None\n",
    "config = dict2attrdict(run.config) if config.wandb.enabled else config\n",
    "print(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.load(Path(config.data.path).expanduser(), \n",
    "               mmap_mode='c' if config.mmap else None)\n",
    "data = data[:, :config.sel_steps]\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_sw = sliding_window_view(data, (data.shape[0], config.lookback + config.horizon, \n",
    "                                 data.shape[-2], data.shape[-1]), \n",
    "                                 (data.shape[0], config.stride, \n",
    "                                  data.shape[-2], data.shape[-1]))\n",
    "samples_per_simulation = data_sw.shape[1]\n",
    "data_sw = data_sw.squeeze().transpose([1,0,2,3,4])\n",
    "data_sw = data_sw.reshape(-1, *data_sw.shape[2:])\n",
    "data_sw.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split and get Normalization statistics from training set (mean and standard deviation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split by simulation\n",
    "splits = RandomSplitter()(data)\n",
    "splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = DensityData(data_sw, lbk=config.lookback, h=config.horizon)\n",
    "train_idxs = calculate_sample_idxs(splits[0], samples_per_simulation)\n",
    "valid_idxs = calculate_sample_idxs(splits[1], samples_per_simulation)\n",
    "len(train_idxs), len(valid_idxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mocat_stats = (np.mean(data[splits[0]]), np.std(data[splits[0]]))\n",
    "mocat_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataloaders\n",
    "train_tl = TfmdLists(train_idxs, DensityTupleTransform(ds))\n",
    "valid_tl = TfmdLists(valid_idxs, DensityTupleTransform(ds))\n",
    "dls = DataLoaders.from_dsets(train_tl, valid_tl, bs=config.bs, device=default_device(),\n",
    "                            after_batch=[Normalize.from_stats(*mocat_stats)] if \\\n",
    "                             config.normalize else None,\n",
    "                            num_workers=config.num_workers)\n",
    "dls.show_batch()\n",
    "foo, bar = dls.one_batch()\n",
    "len(foo), len(bar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func = StackLoss(MSELossFlat())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config.convgru.norm = NormType.Batch if config.convgru.norm == 'batch' else None\n",
    "model = StackUnstack(SimpleModel(**config.convgru)).to(default_device())\n",
    "wandbc = WandbCallback(log_preds=False, log_model=False) if config.wandb.enabled else None\n",
    "cbs = L() + wandbc\n",
    "learn = Learner(dls, model, loss_func=loss_func, cbs=cbs).to_fp16()\n",
    "lr_max = config.lr_max if config.lr_max is not None else learn.lr_find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fit_one_cycle(config.n_epoch, lr_max=lr_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p,t = learn.get_preds()\n",
    "len(p), p[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_res(t, idx, figsize=(8,4)):\n",
    "    density_seq = DensitySeq.create([t[i][idx] for i in range(len(t))])\n",
    "    density_seq.show(figsize=figsize);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = random.randint(0, dls.valid.n)\n",
    "figsize=(12,8)\n",
    "print(k)\n",
    "show_res(t,k, figsize=figsize)\n",
    "show_res(p,k, figsize=figsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "# Print the validation loss and save it in case other notebooks (optuna) wants to\n",
    "# use it for hyperparameter optimization\n",
    "valid_loss = learn.validate()[0] \n",
    "print(valid_loss)\n",
    "%store valid_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the wandb callback to avoid errors when downloading the learner\n",
    "if config.wandb.enabled:\n",
    "    learn.remove_cb(wandbc)\n",
    "\n",
    "# Save locally and in wandb if online and enabled\n",
    "learn.model_dir = config.tmp_folder\n",
    "learn.save('model', with_opt=True)\n",
    "learn.export(f'{config.tmp_folder}/learner.pkl')\n",
    "if run is not None and config.wandb.log_learner:\n",
    "    # Save the learner (all tmp/dls, tmp/model.pth, and tmp/learner.pkl). \n",
    "    run.log_artifact(config.tmp_folder, type='learner', name='density-forecaster')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if run is not None:\n",
    "    run.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
