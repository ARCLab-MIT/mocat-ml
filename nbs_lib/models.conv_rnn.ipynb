{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|default_exp models.conv_rnn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Convolutional Kernels\n",
    "> ConvLSTM and ConvGRU cells and models. Source: https://github.com/tcapelle/moving_mnist/blob/master/nbs/01_models.conv_rnn.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "from fastai.vision.all import *\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "default_device(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conv Coord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class AddCoords(Module):\n",
    "\n",
    "    def __init__(self, with_r=False):\n",
    "        self.with_r = with_r\n",
    "\n",
    "    def forward(self, input_tensor):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            input_tensor: shape(batch, channel, x_dim, y_dim)\n",
    "        \"\"\"\n",
    "        batch_size, _, x_dim, y_dim = input_tensor.size()\n",
    "\n",
    "        xx_channel = torch.arange(x_dim).repeat(1, y_dim, 1)\n",
    "        yy_channel = torch.arange(y_dim).repeat(1, x_dim, 1).transpose(1, 2)\n",
    "\n",
    "        xx_channel = xx_channel.float() / (x_dim - 1)\n",
    "        yy_channel = yy_channel.float() / (y_dim - 1)\n",
    "\n",
    "        xx_channel = xx_channel * 2 - 1\n",
    "        yy_channel = yy_channel * 2 - 1\n",
    "\n",
    "        xx_channel = xx_channel.repeat(batch_size, 1, 1, 1).transpose(2, 3)\n",
    "        yy_channel = yy_channel.repeat(batch_size, 1, 1, 1).transpose(2, 3)\n",
    "\n",
    "        ret = torch.cat([\n",
    "            input_tensor,\n",
    "            xx_channel.type_as(input_tensor),\n",
    "            yy_channel.type_as(input_tensor)], dim=1)\n",
    "\n",
    "        if self.with_r:\n",
    "            rr = torch.sqrt(torch.pow(xx_channel.type_as(input_tensor) - 0.5, 2) + torch.pow(yy_channel.type_as(input_tensor) - 0.5, 2))\n",
    "            ret = torch.cat([ret, rr], dim=1)\n",
    "\n",
    "        return ret\n",
    "\n",
    "\n",
    "@delegates(nn.Conv2d)\n",
    "class CoordConv(Module):\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, **kwargs):\n",
    "        self.addcoords = AddCoords(with_r=True)\n",
    "        in_size = in_channels+2\n",
    "        self.conv = nn.Conv2d(in_size+1, out_channels, kernel_size, **kwargs)\n",
    "\n",
    "    def forward(self, x):\n",
    "        ret = self.addcoords(x)\n",
    "        ret = self.conv(ret)\n",
    "        return ret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ConvGRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class ConvGRU_cell(Module):\n",
    "    def __init__(self, in_ch, out_ch, ks=3, debug=False):\n",
    "        self.in_ch = in_ch\n",
    "        # kernel_size of input_to_state equals state_to_state\n",
    "        self.ks = ks\n",
    "        self.out_ch = out_ch\n",
    "        self.debug = debug\n",
    "        self.padding = (ks - 1) // 2\n",
    "        self.conv1 = nn.Sequential(nn.Conv2d(self.in_ch + self.out_ch,2 * self.out_ch, self.ks, 1,self.padding),\n",
    "                                   nn.GroupNorm(2 * self.out_ch // 8, 2 * self.out_ch))\n",
    "        self.conv2 = nn.Sequential(nn.Conv2d(self.in_ch + self.out_ch,self.out_ch, self.ks, 1, self.padding),\n",
    "                                   nn.GroupNorm(self.out_ch // 8, self.out_ch))\n",
    "\n",
    "    def forward(self, inputs, hidden_state=None):\n",
    "        \"inputs shape: (bs, seq_len, ch, w, h)\"\n",
    "        bs, seq_len, ch, w, h = inputs.shape\n",
    "        if hidden_state is None:\n",
    "            htprev = self.initHidden(bs, self.out_ch, w, h)\n",
    "            if self.debug: print(f'htprev: {htprev.shape}')\n",
    "        else:\n",
    "            htprev = hidden_state\n",
    "        output_inner = []\n",
    "        for index in range(seq_len):\n",
    "            x = inputs[:, index, ...]\n",
    "            combined_1 = torch.cat((x, htprev), 1)  # X_t + H_t-1\n",
    "            gates = self.conv1(combined_1)  # W * (X_t + H_t-1)          \n",
    "            zgate, rgate = torch.split(gates, self.out_ch, dim=1)\n",
    "            z = torch.sigmoid(zgate)\n",
    "            r = torch.sigmoid(rgate)\n",
    "            combined_2 = torch.cat((x, r * htprev),1)\n",
    "            ht = self.conv2(combined_2)\n",
    "            ht = torch.tanh(ht)\n",
    "            htnext = (1 - z) * htprev + z * ht\n",
    "            output_inner.append(htnext)\n",
    "            htprev = htnext\n",
    "        return torch.stack(output_inner, dim=1), htnext\n",
    "    def __repr__(self): return f'ConvGRU_cell(in={self.in_ch}, out={self.out_ch}, ks={self.ks})'\n",
    "    def initHidden(self, bs, ch, w, h): return one_param(self).new_zeros(bs, ch, w, h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ConvGRU_cell(in=32, out=32, ks=3)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cell = ConvGRU_cell(32, 32, debug=True)\n",
    "cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "htprev: torch.Size([2, 32, 36, 99])\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand(2, 7, 32, 36, 99)\n",
    "out, h = cell(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 7, 32, 36, 99])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking sizes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_eq(out.shape, x.shape) \n",
    "test_eq(h.shape, [2,32,36,99])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Should be possible to call with hidden state:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "out2, h2 = cell(out, h)\n",
    "test_eq(h2.shape, [2, 32, 36, 99])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A very nasty module to propagate 2D layers over sequence of images, inspired from Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class TimeDistributed(Module):\n",
    "    \"Applies a module over tdim identically for each step\" \n",
    "    def __init__(self, module, low_mem=False, tdim=1):\n",
    "        self.module = module\n",
    "        self.low_mem = low_mem\n",
    "        self.tdim = tdim\n",
    "        \n",
    "    def forward(self, *args, **kwargs):\n",
    "        \"input x with shape:(bs,seq_len,channels,width,height)\"\n",
    "        if self.low_mem or self.tdim!=1: \n",
    "            return self.low_mem_forward(*args)\n",
    "        else:\n",
    "            # Only support tdim=1\n",
    "            inp_shape = args[0].shape\n",
    "            bs, seq_len = inp_shape[0], inp_shape[1]\n",
    "\n",
    "            # Process non-None arguments only\n",
    "            processed_args = [x.view(bs*seq_len, *x.shape[2:]) for x in args if x is not None]\n",
    "            out = self.module(*processed_args, **kwargs)\n",
    "\n",
    "            out_shape = out.shape\n",
    "            return out.view(bs, seq_len, *out_shape[1:])\n",
    "    \n",
    "    def low_mem_forward(self, *args, **kwargs):                                           \n",
    "        \"input x with shape:(bs,seq_len,channels,width,height)\"\n",
    "        tlen = args[0].shape[self.tdim]\n",
    "        args_split = [torch.unbind(x, dim=self.tdim) for x in args if x is not None]\n",
    "        out = []\n",
    "        for i in range(tlen):\n",
    "            out.append(self.module(*[args[i] for args in args_split]), **kwargs)\n",
    "        return torch.stack(out, dim=self.tdim)\n",
    "    def __repr__(self):\n",
    "        return f'TimeDistributed({self.module})'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dummy(Module):\n",
    "    def __init__(self): pass\n",
    "    def forward(self, x, y): return x+y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TimeDistributed(Conv2d(2, 5, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)))"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tdconv = TimeDistributed(nn.Conv2d(2, 5, 3, 1, 1))\n",
    "tdconv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 10, 5, 8, 8])"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tdconv.low_mem_forward(torch.rand(3, 10, 2, 8, 8)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "tconv2 = TimeDistributed(Dummy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 10, 5])"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tconv2.low_mem_forward(torch.rand(3, 10, 5), torch.rand(3, 10, 5)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 10, 5, 8, 8])"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tdconv(torch.rand(3, 10, 2, 8, 8)).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class Encoder(Module):\n",
    "    def __init__(self, n_in=1, szs=[16,64,96], ks=3, rnn_ks=5, act=nn.ReLU, norm=None, coord_conv=False, debug=False):\n",
    "        self.debug = debug\n",
    "        convs = []\n",
    "        rnns = []\n",
    "        if coord_conv: \n",
    "            self.coord_conv = TimeDistributed(CoordConv(n_in, 8, kernel_size=1))\n",
    "            szs = [8]+szs\n",
    "        else: \n",
    "            self.coord_conv = Lambda(noop)\n",
    "            szs = [n_in]+szs\n",
    "        for ni, nf in zip(szs[0:-1], szs[1:]):\n",
    "            convs.append(ConvLayer(ni, nf, ks=ks, stride=1 if ni==szs[0] else 2, padding=ks//2, act_cls=act, norm_type=norm))\n",
    "            rnns.append(ConvGRU_cell(nf, nf, ks=rnn_ks))\n",
    "        self.convs = nn.ModuleList(TimeDistributed(conv) for conv in convs)\n",
    "        self.rnns = nn.ModuleList(rnns)\n",
    "        \n",
    "    def forward_by_stage(self, inputs, conv, rnn):\n",
    "        if self.debug: \n",
    "            print(f' Layer: {rnn}')\n",
    "            print(' inputs: ', inputs.shape)\n",
    "        inputs = conv(inputs)\n",
    "        if self.debug: print(' after_convs: ', inputs.shape)\n",
    "        outputs_stage, state_stage = rnn(inputs, None)\n",
    "        if self.debug: print(' output_stage: ', outputs_stage.shape)\n",
    "        return outputs_stage, state_stage\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        \"inputs.shape bs,seq_len,1,64,64\"\n",
    "        hidden_states = []\n",
    "        outputs = []\n",
    "        inputs = self.coord_conv(inputs)\n",
    "        for i, (conv, rnn) in enumerate(zip(self.convs, self.rnns)):\n",
    "            if self.debug: print('stage: ',i)\n",
    "            inputs, state_stage = self.forward_by_stage(inputs, conv, rnn)\n",
    "            outputs.append(inputs)\n",
    "            hidden_states.append(state_stage)\n",
    "        return outputs, hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stage:  0\n",
      " Layer: ConvGRU_cell(in=16, out=16, ks=5)\n",
      " inputs:  torch.Size([2, 10, 8, 36, 99])\n",
      " after_convs:  torch.Size([2, 10, 16, 36, 99])\n",
      " output_stage:  torch.Size([2, 10, 16, 36, 99])\n",
      "stage:  1\n",
      " Layer: ConvGRU_cell(in=64, out=64, ks=5)\n",
      " inputs:  torch.Size([2, 10, 16, 36, 99])\n",
      " after_convs:  torch.Size([2, 10, 64, 18, 50])\n",
      " output_stage:  torch.Size([2, 10, 64, 18, 50])\n",
      "stage:  2\n",
      " Layer: ConvGRU_cell(in=96, out=96, ks=5)\n",
      " inputs:  torch.Size([2, 10, 64, 18, 50])\n",
      " after_convs:  torch.Size([2, 10, 96, 9, 25])\n",
      " output_stage:  torch.Size([2, 10, 96, 9, 25])\n"
     ]
    }
   ],
   "source": [
    "enc = Encoder(debug=True, coord_conv=True)\n",
    "densities = torch.rand(2, 10, 1, 36, 99)\n",
    "enc_outs, h = enc(densities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[torch.Size([2, 16, 36, 99]),\n",
       " torch.Size([2, 64, 18, 50]),\n",
       " torch.Size([2, 96, 9, 25])]"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[_.shape for _ in h]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[torch.Size([2, 10, 16, 36, 99]),\n",
       " torch.Size([2, 10, 64, 18, 50]),\n",
       " torch.Size([2, 10, 96, 9, 25])]"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[_.shape for _ in enc_outs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export \n",
    "class UpsampleBlock(Module):\n",
    "    \"A quasi-UNet block, using `PixelShuffle_ICNR upsampling`.\"\n",
    "    @delegates(ConvLayer.__init__)\n",
    "    def __init__(self, in_ch, out_ch, residual=False, blur=False, act_cls=defaults.activation,\n",
    "                 self_attention=False, init=nn.init.kaiming_normal_, norm_type=None, debug=False, **kwargs):\n",
    "        store_attr()\n",
    "        self.shuf = PixelShuffle_ICNR(in_ch, in_ch//2, blur=blur, act_cls=act_cls, norm_type=norm_type)\n",
    "        ni = in_ch//2 if not residual else in_ch//2 + out_ch  #the residual has out_ch (normally in_ch//2)\n",
    "        nf = out_ch\n",
    "        self.conv1 = ConvLayer(ni, nf, act_cls=act_cls, norm_type=norm_type, **kwargs)\n",
    "        self.conv2 = ConvLayer(nf, nf, act_cls=act_cls, norm_type=norm_type,\n",
    "                               xtra=SelfAttention(nf) if self_attention else None, **kwargs)\n",
    "        self.bn = nn.BatchNorm2d(out_ch)\n",
    "        self.relu = act_cls()\n",
    "\n",
    "        apply_init(nn.Sequential(self.conv1, self.conv2), init)\n",
    "    def __repr__(self): return (f'UpsampleBLock(in={self.in_ch}, out={self.out_ch}, blur={self.blur}, residual={self.residual}, '\n",
    "                                f'act={self.act_cls()}, attn={self.self_attention}, norm={self.norm_type})')\n",
    "    \n",
    "    def forward(self, up_in, side_in=None):\n",
    "        up_out = self.shuf(up_in)\n",
    "        if side_in is not None:\n",
    "            if self.debug: print(f'up_out: {up_out.shape}, side_in: {side_in.shape}')\n",
    "            assert up_out.shape[-2:] == side_in.shape[-2::], 'residual shape does not match input'\n",
    "            up_out = torch.cat([up_out, self.bn(side_in)], dim=1)\n",
    "        if self.debug: print(f'up_out: {up_out.shape}')\n",
    "        return self.conv2(self.conv1(up_out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "UpsampleBLock(in=32, out=64, blur=False, residual=False, act=ReLU(), attn=False, norm=None)"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "us = UpsampleBlock(32, 64, residual=False)\n",
    "us"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def conditional_crop_pad(tensor, target_height, target_width):\n",
    "    \"\"\"\n",
    "    Conditionally crops or pads the input tensor to match the target height and width.\n",
    "    Args:\n",
    "        tensor (Tensor): Input tensor of shape (batch_size, seq_len, channels, height, width)\n",
    "        target_height (int): Target height\n",
    "        target_width (int): Target width\n",
    "    Returns:\n",
    "        Tensor: Adjusted tensor\n",
    "    \"\"\"\n",
    "    height, width = tensor.shape[-2], tensor.shape[-1]\n",
    "\n",
    "    # Height adjustment\n",
    "    if height > target_height:\n",
    "        # Crop height\n",
    "        start_h = (height - target_height) // 2\n",
    "        tensor = tensor[:, :, :, start_h:start_h + target_height, :]\n",
    "    elif height < target_height:\n",
    "        # Pad height\n",
    "        padding_h = (target_height - height) // 2\n",
    "        tensor = F.pad(tensor, (0, 0, 0, 0, padding_h, padding_h), \"constant\", 0)\n",
    "\n",
    "    # Width adjustment\n",
    "    if width > target_width:\n",
    "        # Crop width\n",
    "        start_w = (width - target_width) // 2\n",
    "        tensor = tensor[:, :, :, :, start_w:start_w + target_width]\n",
    "    elif width < target_width:\n",
    "        # Pad width\n",
    "        padding_w = (target_width - width) // 2\n",
    "        tensor = F.pad(tensor, (0, 0, padding_w, padding_w, 0, 0), \"constant\", 0)\n",
    "\n",
    "    return tensor\n",
    "\n",
    "# In your Decoder's forward method, you would use this function like so:\n",
    "class Decoder(nn.Module):\n",
    "    # ... (other parts of the Decoder class)\n",
    "\n",
    "    def forward(self, dec_input, hidden_states, enc_outs):\n",
    "        # ... existing logic ...\n",
    "\n",
    "        output = self.head(dec_input)\n",
    "        output_adjusted = conditional_crop_pad(output, 36, 99)\n",
    "        return output_adjusted\n",
    "\n",
    "\n",
    "\n",
    "class Decoder(Module):\n",
    "    def __init__(self, n_out=1, szs=[96,64,16], ks=3, rnn_ks=5, act=nn.ReLU, \n",
    "                 blur=False, attn=False, \n",
    "                 norm=None, debug=False):\n",
    "        self.debug = debug\n",
    "        deconvs = []\n",
    "        rnns = []\n",
    "        szs = szs\n",
    "        for ni, nf in zip(szs[0:-1], szs[1:]):\n",
    "            deconvs.append(UpsampleBlock(ni, nf, blur=blur, self_attention=attn, act_cls=act, norm_type=norm))\n",
    "            rnns.append(ConvGRU_cell(ni, ni, ks=rnn_ks))\n",
    "        \n",
    "        #last layer\n",
    "        deconvs.append(ConvLayer(szs[-1], szs[-1], ks, padding=ks//2, act_cls=act, norm_type=norm))\n",
    "        self.deconvs = nn.ModuleList(TimeDistributed(conv) for conv in deconvs)\n",
    "        self.rnns = nn.ModuleList(rnns)\n",
    "        self.head = TimeDistributed(nn.Conv2d(szs[-1], n_out, kernel_size=1))\n",
    "\n",
    "    def forward_by_stage(self, inputs, state, deconv, rnn, side_in=None):\n",
    "        if self.debug: \n",
    "            print(f' Layer: {rnn}')\n",
    "            print(' inputs:, state: ', inputs.shape, state.shape)\n",
    "        inputs, state_stage = rnn(inputs, state)\n",
    "        if self.debug: \n",
    "            print(' after rnn: ', inputs.shape)\n",
    "            print(f' Layer: {deconv}')\n",
    "            print(f' before Upsample: inputs are {inputs.shape}, side_in is \\\n",
    "                  {side_in.shape if side_in is not None else None}')\n",
    "        outputs_stage = deconv(inputs, side_in)\n",
    "        if self.debug: print(' after_deconvs: ', outputs_stage.shape)\n",
    "        return outputs_stage, state_stage\n",
    "    \n",
    "    def forward(self, dec_input, hidden_states, enc_outs):\n",
    "        # Capture the target spatial dimensions from enc_outs\n",
    "        target_height, target_width = enc_outs[0].shape[-2], enc_outs[0].shape[-1]\n",
    "        if self.debug: print(f'target_height: {target_height}, target_width: {target_width}')\n",
    "\n",
    "        enc_outs = [None]+enc_outs[:-1]\n",
    "        for i, (state, conv, rnn, enc_out) in enumerate(zip(hidden_states[::-1], self.deconvs, self.rnns, enc_outs[::-1])):\n",
    "            if self.debug: print(f'\\nStage: {i} ---------------------------------')\n",
    "            # dec_input, state_stage = self.forward_by_stage(dec_input, state, \n",
    "            #                                                conv, rnn, side_in=enc_out)\n",
    "            dec_input, state_stage = self.forward_by_stage(dec_input, state, \n",
    "                                                           conv, rnn, side_in=None)\n",
    "        output = self.head(dec_input)\n",
    "        # Resize the output to the expected dimensions (padding/cropping)\n",
    "        output_adjusted = conditional_crop_pad(output, target_height, target_width)\n",
    "        return output_adjusted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Decoder(\n",
       "  (deconvs): ModuleList(\n",
       "    (0): TimeDistributed(UpsampleBLock(in=96, out=64, blur=False, residual=False, act=ReLU(), attn=False, norm=None))\n",
       "    (1): TimeDistributed(UpsampleBLock(in=64, out=16, blur=False, residual=False, act=ReLU(), attn=False, norm=None))\n",
       "    (2): TimeDistributed(ConvLayer(\n",
       "      (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): ReLU()\n",
       "    ))\n",
       "  )\n",
       "  (rnns): ModuleList(\n",
       "    (0): ConvGRU_cell(in=96, out=96, ks=5)\n",
       "    (1): ConvGRU_cell(in=64, out=64, ks=5)\n",
       "  )\n",
       "  (head): TimeDistributed(Conv2d(16, 1, kernel_size=(1, 1), stride=(1, 1)))\n",
       ")"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dec = Decoder(debug=True)\n",
    "dec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[torch.Size([2, 10, 16, 36, 99]),\n",
       " torch.Size([2, 10, 64, 18, 50]),\n",
       " torch.Size([2, 10, 96, 9, 25])]"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[_.shape for _ in enc_outs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target_height: 36, target_width: 99\n",
      "\n",
      "Stage: 0 ---------------------------------\n",
      " Layer: ConvGRU_cell(in=96, out=96, ks=5)\n",
      " inputs:, state:  torch.Size([2, 10, 96, 9, 25]) torch.Size([2, 96, 9, 25])\n",
      " after rnn:  torch.Size([2, 10, 96, 9, 25])\n",
      " Layer: TimeDistributed(UpsampleBLock(in=96, out=64, blur=False, residual=False, act=ReLU(), attn=False, norm=None))\n",
      " before Upsample: inputs are torch.Size([2, 10, 96, 9, 25]), side_in is                   None\n",
      " after_deconvs:  torch.Size([2, 10, 64, 18, 50])\n",
      "\n",
      "Stage: 1 ---------------------------------\n",
      " Layer: ConvGRU_cell(in=64, out=64, ks=5)\n",
      " inputs:, state:  torch.Size([2, 10, 64, 18, 50]) torch.Size([2, 64, 18, 50])\n",
      " after rnn:  torch.Size([2, 10, 64, 18, 50])\n",
      " Layer: TimeDistributed(UpsampleBLock(in=64, out=16, blur=False, residual=False, act=ReLU(), attn=False, norm=None))\n",
      " before Upsample: inputs are torch.Size([2, 10, 64, 18, 50]), side_in is                   None\n",
      " after_deconvs:  torch.Size([2, 10, 16, 36, 100])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 10, 1, 36, 99])"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dec(enc_outs[-1], h, enc_outs).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target_height: 36, target_width: 99\n",
      "\n",
      "Stage: 0 ---------------------------------\n",
      " Layer: ConvGRU_cell(in=96, out=96, ks=5)\n",
      " inputs:, state:  torch.Size([2, 10, 96, 9, 25]) torch.Size([2, 96, 9, 25])\n",
      " after rnn:  torch.Size([2, 10, 96, 9, 25])\n",
      " Layer: TimeDistributed(UpsampleBLock(in=96, out=64, blur=False, residual=False, act=ReLU(), attn=False, norm=None))\n",
      " before Upsample: inputs are torch.Size([2, 10, 96, 9, 25]), side_in is                   None\n",
      " after_deconvs:  torch.Size([2, 10, 64, 18, 50])\n",
      "\n",
      "Stage: 1 ---------------------------------\n",
      " Layer: ConvGRU_cell(in=64, out=64, ks=5)\n",
      " inputs:, state:  torch.Size([2, 10, 64, 18, 50]) torch.Size([2, 64, 18, 50])\n",
      " after rnn:  torch.Size([2, 10, 64, 18, 50])\n",
      " Layer: TimeDistributed(UpsampleBLock(in=64, out=16, blur=False, residual=False, act=ReLU(), attn=False, norm=None))\n",
      " before Upsample: inputs are torch.Size([2, 10, 64, 18, 50]), side_in is                   None\n",
      " after_deconvs:  torch.Size([2, 10, 16, 36, 100])\n"
     ]
    }
   ],
   "source": [
    "test_eq(dec(enc_outs[-1], h, enc_outs).shape, densities.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def _unbind_densities(x, dim=1):\n",
    "    \"only unstack densities\"\n",
    "    if isinstance(x, torch.Tensor): \n",
    "        if len(x.shape)>=4:\n",
    "            return x.unbind(dim=dim)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = torch.rand(1,3,36,99)\n",
    "t2 = torch.rand(5)\n",
    "test_eq(_unbind_densities(t), [t[:,i,...] for i in range(3)])\n",
    "test_eq(_unbind_densities(t2), t2)\n",
    "test_eq(_unbind_densities(5.0), 5.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class StackUnstack(Module):\n",
    "    \"Stack together inputs, apply module, unstack output\"\n",
    "    def __init__(self, module, dim=1):\n",
    "        self.dim = dim\n",
    "        self.module = module\n",
    "    \n",
    "    @staticmethod\n",
    "    def unbind_densities(x, dim=1): return _unbind_densities(x, dim)\n",
    "    def forward(self, *args):\n",
    "        inputs = [torch.stack(x, dim=self.dim) for x in args]\n",
    "        outputs = self.module(*inputs)\n",
    "        if isinstance(outputs, (tuple, list)):\n",
    "            return [self.unbind_images(output, dim=self.dim) for output in outputs]\n",
    "        else: return outputs.unbind(dim=self.dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:12: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "<>:14: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "<>:12: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "<>:14: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "/tmp/ipykernel_34364/4113697790.py:12: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "  if self.strategy is 'zero':\n",
      "/tmp/ipykernel_34364/4113697790.py:14: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "  elif self.strategy is 'encoder':\n"
     ]
    }
   ],
   "source": [
    "#export\n",
    "class SimpleModel(Module):\n",
    "    \"Simple Encoder/Decoder module\"\n",
    "    def __init__(self, n_in=1, n_out=1, szs=[16,64,96], ks=3, rnn_ks=5, \n",
    "                 act=nn.ReLU, blur=False, attn=False, norm=None, strategy='zero', \n",
    "                 coord_conv=False, debug=False):\n",
    "        self.strategy = strategy\n",
    "        self.encoder = Encoder(n_in, szs, ks, rnn_ks, act, norm, coord_conv, debug)\n",
    "        self.decoder = Decoder(n_out, szs[::-1], ks, rnn_ks, act, blur, attn, norm, debug)\n",
    "    def forward(self, x):\n",
    "        enc_outs, h = self.encoder(x)\n",
    "        if self.strategy is 'zero':\n",
    "            dec_in = one_param(self).new_zeros(*enc_outs[-1].shape)\n",
    "        elif self.strategy is 'encoder':\n",
    "            dec_in = enc_outs[-1]\n",
    "        return self.decoder(dec_in, h, enc_outs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = StackUnstack(SimpleModel(strategy='zero'))\n",
    "m2 = StackUnstack(SimpleModel(strategy='encoder'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "densities_list = [torch.rand(2,1,36,99) for _ in range(10)]\n",
    "test_eq(len(m(densities_list)), len(densities_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the model can output a list of tensors, we will need to modify the loss function to acomodate this inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def StackLoss(loss_func=MSELossFlat(), axis=-1):\n",
    "    def _inner_loss(x,y):\n",
    "        x = torch.cat(x, axis)\n",
    "        y = torch.cat(y, axis)\n",
    "        return loss_func(x,y)\n",
    "    return _inner_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func = StackLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1, 36, 990])"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cat(densities_list, axis=-1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1, 36, 990])"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cat(m(densities_list), axis=-1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorBase(0.2230, grad_fn=<AliasBackward0>)"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_func(densities_list, m(densities_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class MultiImageDice(Metric):\n",
    "    \"Dice coefficient metric for binary target in segmentation\"\n",
    "    def __init__(self, axis=1): self.axis = axis\n",
    "    def reset(self): self.inter,self.union = 0,0\n",
    "    def accumulate(self, learn):\n",
    "        x = torch.cat(learn.pred, -1)\n",
    "        y = torch.cat(learn.y, -1)\n",
    "#         print(type(x), type(y), x.shape, y.shape)\n",
    "        pred,targ = flatten_check(x.argmax(dim=self.axis), y)\n",
    "        self.inter += (pred*targ).float().sum().item()\n",
    "        self.union += (pred+targ).float().sum().item()\n",
    "\n",
    "    @property\n",
    "    def value(self): return 2. * self.inter/self.union if self.union > 0 else None"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
