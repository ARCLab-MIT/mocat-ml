[
  {
    "objectID": "data.html",
    "href": "data.html",
    "title": "Data",
    "section": "",
    "text": "source\n\napply_sliding_window_3d\n\n apply_sliding_window_3d (data, window_len, stride:Optional[int]=1,\n                          start:int=0, pad_remainder:bool=False,\n                          padding:str='post', padding_value:float=nan,\n                          add_padding_feature:bool=True,\n                          get_x:Union[NoneType,int,list]=None,\n                          get_y:Union[NoneType,int,list]=None,\n                          y_func:Optional[&lt;built-\n                          infunctioncallable&gt;]=None,\n                          output_processor:Optional[&lt;built-\n                          infunctioncallable&gt;]=None, copy:bool=False,\n                          horizon:Union[int,list]=1, seq_first:bool=True,\n                          sort_by:Optional[list]=None,\n                          ascending:bool=True, check_leakage:bool=True)\n\nApply sliding window to 3D data. The data is assumed to have the shape (n_samples, n_features, n_time_steps). Input: data: 3D array window_len: int, length of the sliding window horizon: int, number of time steps to predict kwargs: additional arguments to SlidingWindow Output: X: 3D array, shape (n_samples, n_features, window_len) y: 3D array, shape (n_samples, n_features, horizon)\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ndata\n\n\n\n\n\nwindow_len\nint\n\nlength of lookback window\n\n\nstride\nUnion[None, int]\n1\nn datapoints the window is moved ahead along the sequence. Default: 1. If None, stride=window_len (no overlap)\n\n\nstart\nint\n0\ndetermines the step where the first window is applied: 0 (default) or a given step (int). Previous steps will be discarded.\n\n\npad_remainder\nbool\nFalse\nallows to pad remainder subsequences when the sliding window is applied and get_y == [] (unlabeled data).\n\n\npadding\nstr\npost\n‘pre’ or ‘post’ (optional, defaults to ‘pre’): pad either before or after each sequence. If pad_remainder == False, it indicates the starting point to create the sequence (‘pre’ from the end, and ‘post’ from the beginning)\n\n\npadding_value\nfloat\nnan\nvalue (float) that will be used for padding. Default: np.nan\n\n\nadd_padding_feature\nbool\nTrue\nadd an additional feature indicating whether each timestep is padded (1) or not (0).\n\n\nget_x\nUnion[None, int, list]\nNone\nindices of columns that contain the independent variable (xs). If None, all data will be used as x.\n\n\nget_y\nUnion[None, int, list]\nNone\nindices of columns that contain the target (ys). If None, all data will be used as y. [] means no y data is created (unlabeled data).\n\n\ny_func\nOptional[callable]\nNone\noptional function to calculate the ys based on the get_y col/s and each y sub-window. y_func must be a function applied to axis=1!\n\n\noutput_processor\nOptional[callable]\nNone\noptional function to process the final output (X (and y if available)). This is useful when some values need to be removed.The function should take X and y (even if it’s None) as arguments.\n\n\ncopy\nbool\nFalse\ncopy the original object to avoid changes in it.\n\n\nhorizon\nUnion[int, list]\n1\nnumber of future datapoints to predict (y). If get_y is [] horizon will be set to 0.\n\n\nseq_first\nbool\nTrue\nTrue if input shape (seq_len, n_vars), False if input shape (n_vars, seq_len)\n\n\nsort_by\nOptional[list]\nNone\ncolumn/s used for sorting the array in ascending order\n\n\nascending\nbool\nTrue\nused in sorting\n\n\ncheck_leakage\nbool\nTrue\nchecks if there’s leakage in the output between X and y\n\n\n\n\n# Test\ndata = np.random.rand(100, 3, 10)\nX, y = apply_sliding_window_3d(data, window_len=4, horizon=1, seq_first=False, \n                               stride=None)\ntest_eq(X.shape, (200, 3, 4))\ntest_eq(y.shape, (200, 3, 1))\n\n\nsource\n\n\nDensityData\n\n DensityData (data, lbk, h, gap=0)\n\nArgs: data: The data containing sequences of heatmaps. Shape should be (n_samples, lookback+horizon+gap, height, width). lbk: Number of input frames to use as history (lookback). h: Number of output frames to forecast (horizon).\n\n# Test\ndefault_device(False)\n# NOTE: This is only going to take the first 10 elements for each simulation\ndata = np_load_compressed('TLE_density_10_15x15.npy.gz', path='../example_data')\nlbk = 3\nh = 1\nds = DensityData(data, lbk=lbk, h=h)\nx, y = ds[0]\ntest_eq(x.shape, (lbk, 1, 36, 99))\ntest_eq(y.shape, (h, 1, 36, 99))\nz = torch.cat([x, y], dim=0)\nshow_images(z)\n\n\n\n\n\n# with gap\nlbk = 2\nh = 1\ngap = 1\nds2 = DensityData(data, lbk=lbk, h=h, gap=gap)\nx, y = ds2[0]\ntest_eq(x.shape, (lbk, 1, 36, 99))\ntest_eq(y.shape, (h, 1, 36, 99))\nz = torch.cat([x, y], dim=0)\nshow_images(z)\n\n\n\n\n\n# try negative gaps\nlbk = 4\nh = 4\ngap = -3\nds3 = DensityData(data, lbk=lbk, h=h, gap=gap)\nx, y = ds3[0]\ntest_eq(x.shape, (lbk, 1, 36, 99))\ntest_eq(y.shape, (h, 1, 36, 99))\ntest_eq(x[1:], y[:-1])\nz = torch.cat([x, y], dim=0)\nshow_images(z)\n\n\n\n\n\n# If you want a fixed one\nds_fixed = [ds[i] for i in range(2)]\n\n\n\nAs a Transform\n\nRefactor into fastai compatible transform\n\n\nsource\n\nDensitySeq\n\n DensitySeq (x=None, *rest)\n\nA tuple with elementwise ops and more friendly init behavior\n\nx,y = ds[0]\nDensitySeq.create(x).show(figsize=(5,3), x_disc=RP_DISC, y_disc=AM_DISC)\nDensitySeq.create(y).show(figsize=(5,3), start_epoch=3, x_disc=RP_DISC, \n                          y_disc=AM_DISC, title=\"y\")\n\n[&lt;Axes: title={'center': 'Epoch t+3'}, xlabel='rp [km]', ylabel='Am [m^2/kg]'&gt;]\n\n\n\n\n\n\n\n\n\n# Try only some indices\nDensitySeq.create(x).show(figsize=(5,3), x_disc=RP_DISC, y_disc=AM_DISC, epochs=[2])\n\n[&lt;Axes: title={'center': 'Epoch t+2'}, xlabel='rp [km]', ylabel='Am [m^2/kg]'&gt;]\n\n\n\n\n\n\n# Try creating from preds of 10 elements with horizon 4\nh = 4\npreds = tuple(torch.rand(10, 1, 36, 99) for _ in range(h))\np0 = DensitySeq.from_preds_or_targs(preds, idx=0)\ntest_eq(len(p0), h)\ntest_eq(p0[0].shape, (1, 36, 99))\np0_array = DensitySeq.from_preds_or_targs(preds, idx=0, to_array=True)\ntest_eq(toarray(p0), p0_array)\n\n\n# Convert back to preds\np0_preds = p0.to_preds_or_targs()\ntest_eq(len(p0_preds), h)\ntest_eq(p0_preds[0].shape, (1, 1, 36, 99))\nfor i in range(h):\n    test_eq(p0_preds[i][0], preds[i][0])\n# With array too\np0_preds_array = p0_array.to_preds_or_targs()\ntest_eq(len(p0_preds_array), h)\ntest_eq(p0_preds_array[0].shape, (1, 1, 36, 99))\nfor i in range(h):\n    test_eq(p0_preds_array[i][0], toarray(preds[i][0]))\n\n\nsource\n\n\nDensityTupleTransform\n\n DensityTupleTransform (ds)\n\nDelegates (__call__,decode,setup) to (encodes,decodes,setups) if split_idx matches\n\ntrain_tl = TfmdLists([0,1,2,3,4], DensityTupleTransform(ds))\nvalid_tl = TfmdLists([5,6,7,8,9], DensityTupleTransform(ds))\n\n\nb = train_tl[0]\nexplode_types(b)\n\n{tuple: [{__main__.DensitySeq: [torch.Tensor, torch.Tensor, torch.Tensor]},\n  {__main__.DensitySeq: [torch.Tensor]}]}\n\n\n\ndls = DataLoaders.from_dsets(train_tl, valid_tl, bs=5).to(default_device())\n\n\nb = dls.one_batch()\nexplode_types(b)\n\n{tuple: [{__main__.DensitySeq: [torch.Tensor, torch.Tensor, torch.Tensor]},\n  {__main__.DensitySeq: [torch.Tensor]}]}\n\n\n\nb[0][0].shape\n\ntorch.Size([5, 1, 36, 99])\n\n\n\ndls.show_batch()\n\nAttributeError: AxesImage.set() got an unexpected keyword argument 'ctx'\n\n\n\n\n\n\n\n\n\nsource\n\n\nshow_density_forecast\n\n show_density_forecast (p, idx, figsize=(8, 4), **kwargs)\n\nShow predictions given as a list of tensors. Args: p: list of tensors, each tensor is a prediction of shape (n_samples, n_channels, height, width) idx: int, index of the sample to show\n\n# test\nshow_density_forecast(b[0], 0)"
  },
  {
    "objectID": "models.conv_rnn.html",
    "href": "models.conv_rnn.html",
    "title": "Recurrent Convolutional Kernels",
    "section": "",
    "text": "default_device(0)\n\ndevice(type='cpu')"
  },
  {
    "objectID": "models.conv_rnn.html#conv-coord",
    "href": "models.conv_rnn.html#conv-coord",
    "title": "Recurrent Convolutional Kernels",
    "section": "Conv Coord",
    "text": "Conv Coord\n\nsource\n\nCoordConv\n\n CoordConv (in_channels, out_channels, kernel_size=3,\n            stride:Union[int,Tuple[int,int]]=1,\n            padding:Union[str,int,Tuple[int,int]]=0,\n            dilation:Union[int,Tuple[int,int]]=1, groups:int=1,\n            bias:bool=True, padding_mode:str='zeros', device=None,\n            dtype=None)\n\nSame as nn.Module, but no need for subclasses to call super().__init__\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nin_channels\n\n\n\n\n\nout_channels\n\n\n\n\n\nkernel_size\nint\n3\n\n\n\nstride\ntyping.Union[int, typing.Tuple[int, int]]\n1\n\n\n\npadding\ntyping.Union[str, int, typing.Tuple[int, int]]\n0\n\n\n\ndilation\ntyping.Union[int, typing.Tuple[int, int]]\n1\n\n\n\ngroups\nint\n1\n\n\n\nbias\nbool\nTrue\n\n\n\npadding_mode\nstr\nzeros\nTODO: refine this type\n\n\ndevice\nNoneType\nNone\n\n\n\ndtype\nNoneType\nNone\n\n\n\n\n\nsource\n\n\nAddCoords\n\n AddCoords (with_r=False)\n\nSame as nn.Module, but no need for subclasses to call super().__init__"
  },
  {
    "objectID": "models.conv_rnn.html#convgru",
    "href": "models.conv_rnn.html#convgru",
    "title": "Recurrent Convolutional Kernels",
    "section": "ConvGRU",
    "text": "ConvGRU\n\nsource\n\nConvGRU_cell\n\n ConvGRU_cell (in_ch, out_ch, ks=3, debug=False)\n\nSame as nn.Module, but no need for subclasses to call super().__init__\n\ncell = ConvGRU_cell(32, 32, debug=True)\ncell\n\nConvGRU_cell(in=32, out=32, ks=3)\n\n\n\nx = torch.rand(2, 7, 32, 36, 99)\nout, h = cell(x)\n\nhtprev: torch.Size([2, 32, 36, 99])\n\n\n\nout.shape\n\ntorch.Size([2, 7, 32, 36, 99])\n\n\nChecking sizes:\n\ntest_eq(out.shape, x.shape) \ntest_eq(h.shape, [2,32,36,99])\n\nShould be possible to call with hidden state:\n\nout2, h2 = cell(out, h)\ntest_eq(h2.shape, [2, 32, 36, 99])\n\nA very nasty module to propagate 2D layers over sequence of images, inspired from Keras\n\nsource\n\n\nTimeDistributed\n\n TimeDistributed (module, low_mem=False, tdim=1)\n\nApplies a module over tdim identically for each step\n\nclass Dummy(Module):\n    def __init__(self): pass\n    def forward(self, x, y): return x+y\n\n\ntdconv = TimeDistributed(nn.Conv2d(2, 5, 3, 1, 1))\ntdconv\n\nTimeDistributed(Conv2d(2, 5, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)))\n\n\n\ntdconv.low_mem_forward(torch.rand(3, 10, 2, 8, 8)).shape\n\ntorch.Size([3, 10, 5, 8, 8])\n\n\n\ntconv2 = TimeDistributed(Dummy())\n\n\ntconv2.low_mem_forward(torch.rand(3, 10, 5), torch.rand(3, 10, 5)).shape\n\ntorch.Size([3, 10, 5])\n\n\n\ntdconv(torch.rand(3, 10, 2, 8, 8)).shape\n\ntorch.Size([3, 10, 5, 8, 8])"
  },
  {
    "objectID": "models.conv_rnn.html#encoder",
    "href": "models.conv_rnn.html#encoder",
    "title": "Recurrent Convolutional Kernels",
    "section": "Encoder",
    "text": "Encoder\n\nsource\n\nEncoder\n\n Encoder (n_in=1, szs=[16, 64, 96], ks=3, rnn_ks=5, act=&lt;class\n          'torch.nn.modules.activation.ReLU'&gt;, norm=None,\n          coord_conv=False, debug=False)\n\nSame as nn.Module, but no need for subclasses to call super().__init__\n\nenc = Encoder(debug=True, coord_conv=True)\ndensities = torch.rand(2, 10, 1, 36, 99)\nenc_outs, h = enc(densities)\n\nstage:  0\n Layer: ConvGRU_cell(in=16, out=16, ks=5)\n inputs:  torch.Size([2, 10, 8, 36, 99])\n after_convs:  torch.Size([2, 10, 16, 36, 99])\n output_stage:  torch.Size([2, 10, 16, 36, 99])\nstage:  1\n Layer: ConvGRU_cell(in=64, out=64, ks=5)\n inputs:  torch.Size([2, 10, 16, 36, 99])\n after_convs:  torch.Size([2, 10, 64, 18, 50])\n output_stage:  torch.Size([2, 10, 64, 18, 50])\nstage:  2\n Layer: ConvGRU_cell(in=96, out=96, ks=5)\n inputs:  torch.Size([2, 10, 64, 18, 50])\n after_convs:  torch.Size([2, 10, 96, 9, 25])\n output_stage:  torch.Size([2, 10, 96, 9, 25])\n\n\n\n[_.shape for _ in h]\n\n[torch.Size([2, 16, 36, 99]),\n torch.Size([2, 64, 18, 50]),\n torch.Size([2, 96, 9, 25])]\n\n\n\n[_.shape for _ in enc_outs]\n\n[torch.Size([2, 10, 16, 36, 99]),\n torch.Size([2, 10, 64, 18, 50]),\n torch.Size([2, 10, 96, 9, 25])]\n\n\n\nsource\n\n\nUpsampleBlock\n\n UpsampleBlock (in_ch, out_ch, residual=False, blur=False, act_cls=&lt;class\n                'torch.nn.modules.activation.ReLU'&gt;, self_attention=False,\n                init=&lt;function kaiming_normal_&gt;, norm_type=None,\n                debug=False, ks=3, stride=1, padding=None, bias=None,\n                ndim=2, bn_1st=True, transpose=False, xtra=None,\n                bias_std=0.01, dilation:Union[int,Tuple[int,int]]=1,\n                groups:int=1, padding_mode:str='zeros', device=None,\n                dtype=None)\n\nA quasi-UNet block, using PixelShuffle_ICNR upsampling.\n\nus = UpsampleBlock(32, 64, residual=False)\nus\n\nUpsampleBLock(in=32, out=64, blur=False, residual=False, act=ReLU(), attn=False, norm=None)\n\n\n\nsource\n\n\nDecoder\n\n Decoder (n_out=1, szs=[96, 64, 16], ks=3, rnn_ks=5, act=&lt;class\n          'torch.nn.modules.activation.ReLU'&gt;, blur=False, attn=False,\n          norm=None, debug=False)\n\nSame as nn.Module, but no need for subclasses to call super().__init__\n\nsource\n\n\nDecoder\n\n Decoder (n_out=1, szs=[96, 64, 16], ks=3, rnn_ks=5, act=&lt;class\n          'torch.nn.modules.activation.ReLU'&gt;, blur=False, attn=False,\n          norm=None, debug=False)\n\nSame as nn.Module, but no need for subclasses to call super().__init__\n\nsource\n\n\nconditional_crop_pad\n\n conditional_crop_pad (tensor, target_height, target_width)\n\nConditionally crops or pads the input tensor to match the target height and width. Args: tensor (Tensor): Input tensor of shape (batch_size, seq_len, channels, height, width) target_height (int): Target height target_width (int): Target width Returns: Tensor: Adjusted tensor\n\ndec = Decoder(debug=True)\ndec\n\nDecoder(\n  (deconvs): ModuleList(\n    (0): TimeDistributed(UpsampleBLock(in=96, out=64, blur=False, residual=False, act=ReLU(), attn=False, norm=None))\n    (1): TimeDistributed(UpsampleBLock(in=64, out=16, blur=False, residual=False, act=ReLU(), attn=False, norm=None))\n    (2): TimeDistributed(ConvLayer(\n      (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (1): ReLU()\n    ))\n  )\n  (rnns): ModuleList(\n    (0): ConvGRU_cell(in=96, out=96, ks=5)\n    (1): ConvGRU_cell(in=64, out=64, ks=5)\n  )\n  (head): TimeDistributed(Conv2d(16, 1, kernel_size=(1, 1), stride=(1, 1)))\n)\n\n\n\n[_.shape for _ in enc_outs]\n\n[torch.Size([2, 10, 16, 36, 99]),\n torch.Size([2, 10, 64, 18, 50]),\n torch.Size([2, 10, 96, 9, 25])]\n\n\n\ndec(enc_outs[-1], h, enc_outs).shape\n\ntarget_height: 36, target_width: 99\n\nStage: 0 ---------------------------------\n Layer: ConvGRU_cell(in=96, out=96, ks=5)\n inputs:, state:  torch.Size([2, 10, 96, 9, 25]) torch.Size([2, 96, 9, 25])\n after rnn:  torch.Size([2, 10, 96, 9, 25])\n Layer: TimeDistributed(UpsampleBLock(in=96, out=64, blur=False, residual=False, act=ReLU(), attn=False, norm=None))\n before Upsample: inputs are torch.Size([2, 10, 96, 9, 25]), side_in is                   None\n after_deconvs:  torch.Size([2, 10, 64, 18, 50])\n\nStage: 1 ---------------------------------\n Layer: ConvGRU_cell(in=64, out=64, ks=5)\n inputs:, state:  torch.Size([2, 10, 64, 18, 50]) torch.Size([2, 64, 18, 50])\n after rnn:  torch.Size([2, 10, 64, 18, 50])\n Layer: TimeDistributed(UpsampleBLock(in=64, out=16, blur=False, residual=False, act=ReLU(), attn=False, norm=None))\n before Upsample: inputs are torch.Size([2, 10, 64, 18, 50]), side_in is                   None\n after_deconvs:  torch.Size([2, 10, 16, 36, 100])\n\n\ntorch.Size([2, 10, 1, 36, 99])\n\n\n\ntest_eq(dec(enc_outs[-1], h, enc_outs).shape, densities.shape)\n\ntarget_height: 36, target_width: 99\n\nStage: 0 ---------------------------------\n Layer: ConvGRU_cell(in=96, out=96, ks=5)\n inputs:, state:  torch.Size([2, 10, 96, 9, 25]) torch.Size([2, 96, 9, 25])\n after rnn:  torch.Size([2, 10, 96, 9, 25])\n Layer: TimeDistributed(UpsampleBLock(in=96, out=64, blur=False, residual=False, act=ReLU(), attn=False, norm=None))\n before Upsample: inputs are torch.Size([2, 10, 96, 9, 25]), side_in is                   None\n after_deconvs:  torch.Size([2, 10, 64, 18, 50])\n\nStage: 1 ---------------------------------\n Layer: ConvGRU_cell(in=64, out=64, ks=5)\n inputs:, state:  torch.Size([2, 10, 64, 18, 50]) torch.Size([2, 64, 18, 50])\n after rnn:  torch.Size([2, 10, 64, 18, 50])\n Layer: TimeDistributed(UpsampleBLock(in=64, out=16, blur=False, residual=False, act=ReLU(), attn=False, norm=None))\n before Upsample: inputs are torch.Size([2, 10, 64, 18, 50]), side_in is                   None\n after_deconvs:  torch.Size([2, 10, 16, 36, 100])"
  },
  {
    "objectID": "models.conv_rnn.html#model",
    "href": "models.conv_rnn.html#model",
    "title": "Recurrent Convolutional Kernels",
    "section": "Model",
    "text": "Model\n\nt = torch.rand(1,3,36,99)\nt2 = torch.rand(5)\ntest_eq(_unbind_densities(t), [t[:,i,...] for i in range(3)])\ntest_eq(_unbind_densities(t2), t2)\ntest_eq(_unbind_densities(5.0), 5.0)\n\n\nsource\n\nStackUnstack\n\n StackUnstack (module, dim=1)\n\nStack together inputs, apply module, unstack output\n\nsource\n\n\nSimpleModel\n\n SimpleModel (n_in=1, n_out=1, szs=[16, 64, 96], ks=3, rnn_ks=5,\n              act=&lt;class 'torch.nn.modules.activation.ReLU'&gt;, blur=False,\n              attn=False, norm=None, strategy='zero', coord_conv=False,\n              debug=False)\n\nSimple Encoder/Decoder module\n\nm = StackUnstack(SimpleModel(strategy='zero'))\nm2 = StackUnstack(SimpleModel(strategy='encoder'))\n\n\ndensities_list = [torch.rand(2,1,36,99) for _ in range(10)]\ntest_eq(len(m(densities_list)), len(densities_list))"
  },
  {
    "objectID": "models.conv_rnn.html#loss",
    "href": "models.conv_rnn.html#loss",
    "title": "Recurrent Convolutional Kernels",
    "section": "Loss",
    "text": "Loss\nAs the model can output a list of tensors, we will need to modify the loss function to acomodate this inputs.\n\nsource\n\nStackLoss\n\n StackLoss (loss_func=FlattenedLoss of MSELoss(), axis=-1)\n\nBase class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n:ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool\n\nloss_func = StackLoss()\n\n\ntorch.cat(densities_list, axis=-1).shape\n\ntorch.Size([2, 1, 36, 990])\n\n\n\ntorch.cat(m(densities_list), axis=-1).shape\n\ntorch.Size([2, 1, 36, 990])\n\n\n\nloss_func(densities_list, m(densities_list))\n\nTensorBase(0.5664, grad_fn=&lt;AliasBackward0&gt;)\n\n\n\nlen(densities_list), densities_list[0].shape\n\n(10, torch.Size([2, 1, 36, 99]))\n\n\n\nsource\n\n\nPartialStackLoss\n\n PartialStackLoss (idxs, loss_func=FlattenedLoss of MSELoss(), axis=-1)\n\nStackLoss but only in a subset of the elements of the list\n\n# test\nloss_func = PartialStackLoss([0])\nloss_func(densities_list, m(densities_list))\n\nTensorBase(0.9245, grad_fn=&lt;AliasBackward0&gt;)\n\n\n\n# Test that with the whole list of idxs it is the same as StackLoss\nloss_func = PartialStackLoss(list(range(len(densities_list))))\ntest_eq(loss_func(densities_list, m(densities_list)), \n        StackLoss()(densities_list, m(densities_list)))\n\n\nsource\n\n\nMultiImageDice\n\n MultiImageDice (axis=1)\n\nDice coefficient metric for binary target in segmentation"
  },
  {
    "objectID": "models.utils.html",
    "href": "models.utils.html",
    "title": "Model utils",
    "section": "",
    "text": "import sys\nsys.path.append('..')\nfrom tsai.basics import *\n\n\nsource\n\nstack_density_list_as_preds_targs\n\n stack_density_list_as_preds_targs (l)\n\n\n# test with a list of DensitySeqs\ndata = np_load_compressed('TLE_density_10_15x15.npy.gz', \n                        path='../example_data')\nds = DensityData(data, lbk=4, h=4, gap=0)\ntl = TfmdLists(range(len(ds)), DensityTupleTransform(ds))\nl = [y for _,y in tl]\nfinal_preds = stack_density_list_as_preds_targs(l)\ntest_eq(len(final_preds), 4)\ntest_eq(final_preds[0].shape, (10, 1, 36, 99))\nfor i in range(len(ds)):\n    test_eq(l[i], DensitySeq.from_preds_or_targs(final_preds, i))\nlen(final_preds), final_preds[0].shape\n\n(4, torch.Size([10, 1, 36, 99]))\n\n\n\ndata[:,4:8].shape\n\n(10, 4, 36, 99)\n\n\n\nsource\n\n\nLearner.get_preds_iterative\n\n Learner.get_preds_iterative (dl, n_iter=1, track_losses=False,\n                              with_input=False, ds_idx:int=1, act=None,\n                              inner:bool=False, reorder:bool=True, cbs:Uni\n                              on[fastai.callback.core.Callback,collections\n                              .abc.MutableSequence,NoneType]=None,\n                              save_preds:pathlib.Path=None,\n                              save_targs:pathlib.Path=None,\n                              with_preds:bool=True, with_targs:bool=True,\n                              concat_dim:int=0, pickle_protocol:int=2)\n\nCall get preds iteratively on a dataloader with a DensityTupleTransform TODO: Crashes if inner=True (kwargs), so it will produce invisible progress bars\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ndl\nNoneType\nNone\nDataLoader to use for predictions, defaults to ds_idx=1 if None\n\n\nn_iter\nint\n1\n\n\n\ntrack_losses\nbool\nFalse\n\n\n\nwith_input\nbool\nFalse\nReturn inputs with predictions\n\n\nds_idx\nint\n1\nDataLoader to use for predictions if dl is None. 0: train. 1: valid\n\n\nact\nNoneType\nNone\nApply activation to predictions, defaults to self.loss_func’s activation\n\n\ninner\nbool\nFalse\nIf False, create progress bar, show logger, use temporary cbs\n\n\nreorder\nbool\nTrue\nReorder predictions on dataset indicies, if applicable\n\n\ncbs\nCallback | MutableSequence | None\nNone\nTemporary Callbacks to apply during prediction\n\n\nsave_preds\nPath\nNone\nPath to save predictions\n\n\nsave_targs\nPath\nNone\nPath to save targets\n\n\nwith_preds\nbool\nTrue\nWhether to return predictions\n\n\nwith_targs\nbool\nTrue\nWhether to return targets\n\n\nconcat_dim\nint\n0\nDimension to concatenate returned tensors\n\n\npickle_protocol\nint\n2\nPickle protocol used to save predictions and targets\n\n\n\n\ndefault_device(0)\n# TODO: test for gaps 0, pºositive and negative\nlbk = 4\nh = 4\nn_iter= 4\ngap = 0\ndata = np_load_compressed('TLE_density_10_15x15.npy.gz', \n                        path='../example_data')\nds = DensityData(data, lbk=lbk, h=h, gap=gap)\ntl = TfmdLists(range(len(ds)), DensityTupleTransform(ds))\ndls = tl.dataloaders(bs=32, shuffle=False, num_workers=0)\nlearn = Learner(dls=dls, \n                model=StackUnstack(SimpleModel()).to(default_device()),\n                loss_func=StackLoss())\ninp, p,t,losses = learn.get_preds_iterative(dl=dls[0], n_iter=n_iter, \n                                        track_losses=True, with_input=True);\ntest_eq(len(p), h)\ntest_eq(p[0].shape, [10, 1, 36, 99])\ntest_eq(losses.shape[0], n_iter)\nassert all_equal(torch.stack(inp, dim=learn.dim).squeeze(0), \n        torch.stack([torch.stack(learn.dls.itemgot()[x][0], dim=0) for x in range(dls[0].n)], dim=0))\n# Compare with the direct prediction made with a gap\nds_gap = copy(ds)\nds_gap.gap += (n_iter-1)*h\ndl_gap = dls.new(TfmdLists(range(len(ds_gap)), DensityTupleTransform(ds_gap)))\np2,t2 = learn.get_preds(dl=dl_gap)\nloss2 = learn.loss_func(p2,t2).item()\ntest_ne(p, p2)\ntest_eq(t, t2)\n\n\n\n\n\n\n\n\n(10, 8, 36, 99)\n(10, 8, 36, 99)\n(10, 8, 36, 99)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nsource\n\n\nLearner.get_individual_losses\n\n Learner.get_individual_losses (p, t)\n\nGet the loss for each element given predictions and targets computed in learn.get_preds\n\n# test\nindividual_losses = learn.get_individual_losses(p,t)\ntest_eq(len(individual_losses), len(ds))\ntest_close(individual_losses.mean(), learn.loss_func(p,t), eps=1e-2)\n\n\nsource\n\n\nLearner.predict_at\n\n Learner.predict_at (idx, ds_idx=1, ds=None, with_input=False)\n\nPredict at a given index on a given dataset, or in the learner’s\n\n# Test\np0, t0 = learn.predict_at(0, ds_idx=0)\ntest_eq(len(p0), h)\ntest_eq(p0[0].shape, [1, 1, 36, 99])\nassert all_equal(torch.stack(t0, dim=learn.dim).squeeze(0),\n            torch.stack(learn.dls.itemgot()[0][1], dim=0))\n\ninp0,p0,t0  = learn.predict_at(0, ds_idx=0, with_input=True)\ntest_eq(len(p0), h)\ntest_eq(p0[0].shape, [1, 1, 36, 99])\ntest_eq(t0[0].shape, [1, 1, 36, 99])\ntest_eq(inp0[0].shape, [1, 1, 36, 99])\nassert all_equal(torch.stack(inp0, dim=learn.dim).squeeze(0), \n          torch.stack(learn.dls.itemgot()[0][0], dim=0))\nassert all_equal(torch.stack(t0, dim=learn.dim).squeeze(0),\n            torch.stack(learn.dls.itemgot()[0][1], dim=0))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nsource\n\n\nLearner.show_preds_at\n\n Learner.show_preds_at (idx, p=None, t=None, inp=None, with_input=None,\n                        with_targets=False, titles=['Input', 'Prediction',\n                        'Target'], start_epoch=0, x_disc=None,\n                        y_disc=None, figsize=(4, 3), epochs=None)\n\nShow predictions at a given index\n\n# Test\nlbk = 4\nh = 4\nn_iter= 2\ngap = -3\ndata = np_load_compressed('TLE_density_10_15x15.npy.gz', \n                        path='../example_data')\nds = DensityData(data, lbk=lbk, h=h, gap=gap)\ntl = TfmdLists(range(len(ds)), DensityTupleTransform(ds))\ndls = tl.dataloaders(bs=32, shuffle=False, num_workers=0)\nlearn = Learner(dls=dls, \n                model=StackUnstack(SimpleModel()).to(default_device()),\n                loss_func=StackLoss())\ninps, p,t = learn.get_preds_iterative(dl=dls[0], n_iter=n_iter, with_input=True)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nlearn.show_preds_at(0, p=p, t=t, inp=inps)"
  },
  {
    "objectID": "utils.html",
    "href": "utils.html",
    "title": "Utils",
    "section": "",
    "text": "source\n\nconvert_uuids_to_indices\n\n convert_uuids_to_indices ()\n\n\nsource\n\n\npercent_rel_error_\n\n percent_rel_error_ (N, N_mc, N_mc_t0)\n\nReturn the percentage relative error with respect to the MC simulation See Eq. 23 of the paper by Giudici et al. (2023), ‘Space debris density propagation through a vinite volume method’ Input: N: Predicted number of in-orbit elements over time N_mc: Number of in-orbit elements from the MC simulation over time N_mc_t0: Number of in-orbit elements from the MC simulation at t0 Output: The percentage relative error (float)\n\n# Test\nN = np.array([1, 2, 3, 4, 5])\nN_mc = np.array([1, 2, 3, 4, 5])\nN_mc_t0 = N_mc[0]\nres = percent_rel_error_(N, N_mc, N_mc_t0)\ntest_eq(res, np.zeros_like(res))\n\n# Test 2\nN = np.array([1, 2, 3, 4, 5])\nN_mc = np.array([2, 4, 6, 8, 10])\nN_mc_t0 = N_mc[0]\nres = percent_rel_error_(N, N_mc, N_mc_t0)\ntest_eq(res, np.array([ 50., 100., 150., 200., 250.]))\n\n\nsource\n\n\ncalculate_sample_idxs\n\n calculate_sample_idxs (simulation_idxs, samples_per_sim)\n\n\n# Test\nsim_data = np.random.rand(10, 4, 36, 99) # sims x samples x h x w\nsimulation_idxs = [1, 3]\nsamples_per_sim = sim_data.shape[1]\nsample_idxs = calculate_sample_idxs(simulation_idxs, samples_per_sim)\nsample_idxs\n\n[4, 5, 6, 7, 12, 13, 14, 15]"
  }
]